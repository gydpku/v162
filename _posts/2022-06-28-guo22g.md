---
title: Online Continual Learning through Mutual Information Maximization
booktitle: Proceedings of the 39th International Conference on Machine Learning
abstract: This paper proposed a new online continual learning approach called OCM
  based on <em>mutual information</em> (MI) <em>maximization</em>. It achieves two
  objectives that are critical in dealing with catastrophic forgetting (CF). (1)
  It reduces feature bias caused by cross entropy (CE) as CE learns only discriminative
  features for each task, but these features may not be discriminative for another
  task. To learn a new task well, the network parameters learned before have to be
  modified, which causes CF. The new approach encourages the learning of each task
  to make use of the full features of the task training data. (2) It encourages preservation
  of the previously learned knowledge when training a new batch of incrementally arriving
  data. Empirical evaluation shows that OCM substantially outperforms the latest
  online CL baselines. For example, for CIFAR10, OCM improves the accuracy of the
  best baseline by 13.1% from 64.1% (baseline) to 77.2% (OCM).The code is publicly
  available at https://github.com/gydpku/OCM.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: guo22g
month: 0
tex_title: Online Continual Learning through Mutual Information Maximization
firstpage: 8109
lastpage: 8126
page: 8109-8126
order: 8109
cycles: false
bibtex_author: Guo, Yiduo and Liu, Bing and Zhao, Dongyan
author:
- given: Yiduo
  family: Guo
- given: Bing
  family: Liu
- given: Dongyan
  family: Zhao
date: 2022-06-28
address:
container-title: Proceedings of the 39th International Conference on Machine Learning
volume: '162'
genre: inproceedings
issued:
  date-parts:
  - 2022
  - 6
  - 28
pdf: https://proceedings.mlr.press/v162/guo22g/guo22g.pdf
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
